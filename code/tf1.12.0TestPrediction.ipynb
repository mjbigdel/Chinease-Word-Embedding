{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tf1.12.0TestPrediction.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"code","id":"hn2V93X808iH","outputId":"2929b575-bbf2-43de-c96a-bdea8f528399","executionInfo":{"status":"ok","timestamp":1555764309974,"user_tz":-120,"elapsed":33499,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":615}},"cell_type":"code","source":["!pip install tensorflow==1.12.0\n","# TensorFlow and tf.keras\n","# its just working with tensorflow 1.13.1, with others has problem\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.12.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n","\u001b[K    100% |████████████████████████████████| 83.1MB 359kB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.7.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.16.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.15.0)\n","Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12.0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n","\u001b[K    100% |████████████████████████████████| 3.1MB 7.2MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.33.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.0.9)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.2.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.0.7)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (1.11.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (3.7.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.0) (0.7.1)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (0.15.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (3.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12.0) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12.0) (40.9.0)\n","Installing collected packages: tensorboard, tensorflow\n","  Found existing installation: tensorboard 1.13.1\n","    Uninstalling tensorboard-1.13.1:\n","      Successfully uninstalled tensorboard-1.13.1\n","  Found existing installation: tensorflow 1.13.1\n","    Uninstalling tensorflow-1.13.1:\n","      Successfully uninstalled tensorflow-1.13.1\n","Successfully installed tensorboard-1.12.2 tensorflow-1.12.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["tensorflow"]}}},"metadata":{"tags":[]}}]},{"metadata":{"colab_type":"code","id":"l_nAMKMN3Lkg","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1555969204272,"user_tz":-120,"elapsed":2035,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"id":"cXO7IbXUVspf","outputId":"315f26b6-f487-47b8-cbbc-f25b9b011017","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.13.1\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1556046832090,"user_tz":-120,"elapsed":3280,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"id":"NeNCvsKU2i-V","outputId":"6c83587a-c356-43ed-91df-d0eb428fa048","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1556046839343,"user_tz":-120,"elapsed":5746,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"id":"BB3PsCWK5R-T","outputId":"47792546-19de-40a3-b840-eb757325ea10","colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["!ls \"/content/gdrive/My Drive/NLPHW1\"\n","root_path = '/content/gdrive/My Drive/NLPHW1/'\n","print(root_path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" cityu_best_models\t\t      logging\n","'Copy of LastCode_NLPHW1 (2).ipynb'   msr_best_models\n"," Datasets\t\t\t      pku_best_models\n"," embedding\t\t\t      tf1.12.0TestPrediction.ipynb\n","'LastCode_NLPHW1 (2).ipynb'\n","/content/gdrive/My Drive/NLPHW1/\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"_9SeAA4R1apd"},"cell_type":"markdown","source":["**imports and basic definitions**"]},{"metadata":{"colab_type":"code","id":"Xx6SdSVr9vCP","colab":{}},"cell_type":"code","source":["import os\n","import re\n","import errno\n","import pickle\n","import numpy as np\n","from collections import Counter\n","import codecs\n","import tensorflow as tf\n","from tensorflow.contrib import layers\n","from tensorflow.contrib import crf\n","   \n","#from hanziconv import HanziConv\n","\n","START_SENT = \"گ\"\n","END_SENT = \"چ\"\n","UNK = \"ژ\"\n","PAD = \"پ\"\n","\n","TAGB,TAGI,TAGE,TAGS=0,1,2,3\n","\n","rNUM = '(-|\\+)?\\d+((\\.|·)\\d+)?%?'\n","rENG = '[A-Za-z_.]+'\n","\n","def make_sure_path_exists(path):\n","    try:\n","        os.makedirs(path)\n","    except OSError as exception:\n","        if exception.errno != errno.EEXIST:\n","            raise\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"MQOE6XSr0iFO","colab":{}},"cell_type":"code","source":["def remove_spaces(src_file, des_file, encode='UTF-8'):\n","    with open(src_file, encoding=encode) as src_file, open(des_file, 'w', encoding=encode) as des_file:\n","        for line in src_file: \n","            # cleaning input                     \n","            sent_words = line.split()                             \n","            sentence = ''.join(sent_words) + '\\n'\n","            des_file.write(sentence)     \n","\n","# Function for cleaning file, make charachters better and spaces standard space.\n","def clean_data(src_file, des_file, encode='UTF-8'):\n","    with open(src_file, encoding=encode) as src_file, open(des_file, 'w', encoding=encode) as des_file:\n","        sentences=[]\n","        for line in src_file: \n","            # cleaning input\n","            rstring = \"\"\n","            for uchar in line:\n","                inside_code = ord(uchar)\n","                if inside_code == 12288:\n","                    inside_code = 32\n","                elif 65281 <= inside_code <= 65374:\n","                    inside_code -= 65248\n","            \n","                rstring += chr(inside_code)      \n","            \n","            sent_words = rstring.split()\n","            \n","            new_sent_words = []\n","            \n","            for word in sent_words:\n","                word = re.sub('\\s+', '', word, flags=re.U)    \n","                new_sent_words.append(word)\n","            \n","            sentences.append(new_sent_words)\n","        for sent_num in range(len(sentences)):            \n","#             if(sent_num != len(sentences)-1):\n","            des_file.write(' '.join(sentences[sent_num]) + '\\n') \n","#             else:\n","#                 des_file.write(' '.join(sentences[sent_num])) \n","\n","# getting test data function with unigram, bigram and trigrams\n","def get_test_data(filename,ngrams2id, usebigram, usetrigram,usefourgram):   \n","    x,y=[],[]\n","    with codecs.open(filename,'r','utf-8') as f:\n","        # for every line we split and then tag every word with proper id.\n","        for line in f:   \n","            line_x = []\n","            # for every line we remove spaces and then assume every 5 (2 left char and 2 right char)\n","            # char as a new context, for every context we also calculate bigram and trigram and add them too\n","            line=re.sub(u'\\s+','',line.strip())\n","            contexs=window(line)\n","            for contex in contexs:\n","                charx=[]\n","                #contex window\n","                charx.extend([ngrams2id.get(c,ngrams2id[UNK]) for c in contex])\n","                #bigram feature\n","                if usebigram:\n","                    charx.extend([ngrams2id.get(bigram, ngrams2id[UNK]) for bigram in ngram(contex,2)])\n","                if usetrigram:\n","                    charx.extend([ngrams2id.get(trigram, ngrams2id[UNK]) for trigram in ngram(contex,3)])\n","                if usefourgram:\n","                    charx.extend([ngrams2id.get(fourgram, ngrams2id[UNK]) for fourgram in ngram(contex,4)])\n","                line_x.append(charx)\n","            x.append(line_x)\n","    return x\n","\n","# getting ngrams function\n","def ngram(ustr,n=2):\n","    ngram_list=[]\n","    for i in range(len(ustr)-n+1):\n","        ngram_list.append(ustr[i:i+n])\n","    return ngram_list\n","\n","\n","# window return sentence as list of 5 length context, for every word we are \n","# gettnig 4 neighbour, 2 left and 2 right for having more training data.\n","def window(ustr,left=2,right=2):\n","    sent=''\n","    for i in range(left):\n","        sent+=START_SENT\n","    sent+=ustr\n","    for i in range(right):\n","        sent+=END_SENT\n","    windows=[]\n","    for i in range(len(ustr)):\n","        windows.append(sent[i:i+left+right+1])\n","    return windows\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"c-mnVknVIGBu","colab":{}},"cell_type":"code","source":["def prepare_files_test(all_datasets):    \n","    remove_spaces(root_path+'Datasets/gold/{}_test_gold.utf8'.format(all_datasets), root_path+'Datasets/gold/no_space_{}_test_gold.utf8'.format(all_datasets))\n","\n","    clean_data(root_path+'Datasets/gold/no_space_{}_test_gold.utf8'.format(all_datasets), root_path+'Datasets/cleaned_data/{}/raw/cleaned_no_space_test.utf8'.format(all_datasets))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TewfMfCXcfSW","colab_type":"code","colab":{}},"cell_type":"code","source":["all_datasets = 'pku'\n","prepare_files_test(all_datasets)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uEAdsc34a85t","colab_type":"code","colab":{}},"cell_type":"code","source":["# use pretrained embeddings function, for chars we are using pretrain file and for bigram, trigram \n","# and fourgram we are using mean of embeddings of all unigrams of them\n","def reading_pretrained_embeddings(filename, ngrams2id):\n","    # Reading Pretrained Embeddings from file\n","    pretrain_embeddigs = {}\n","    with codecs.open(filename, \"r\", \"utf-8\") as f:\n","        for line in f:\n","            pre_train = line.split()\n","            if len(pre_train) > 2:\n","                word = pre_train[0]\n","                if word in ngrams2id:\n","                    vec = pre_train[1:]\n","                    pretrain_embeddigs[word] = vec                \n","    \n","    print(\"pretraine embedding files reading finished ...\")\n","    # making embeddings for all ngrams2id.\n","    embedding_dim = len(next(iter(pretrain_embeddigs.values())))\n","    out_of_vocab = 0\n","    out = np.ones((len(ngrams2id), embedding_dim))\n","    for ngram in ngrams2id.keys():\n","        if len(ngram) == 1:\n","            if ngram in pretrain_embeddigs.keys():        \n","                out[ngrams2id[ngram]]=np.array(pretrain_embeddigs[ngram])\n","            else:\n","                out_of_vocab+=1\n","                np.random.uniform(-0.8, 0.8, embedding_dim)\n","    for ngram in ngrams2id.keys():        \n","        #embedding for bigrams\n","        if len(ngram) == 2:            \n","            out[ngrams2id[ngram]]= (out[ngrams2id[ngram[0]]]+out[ngrams2id[ngram[1]]])/2\n","        #embedding for trigrams\n","        if len(ngram) == 3:\n","            out[ngrams2id[ngram]]= (out[ngrams2id[ngram[0]]]+out[ngrams2id[ngram[1]]]+out[ngrams2id[ngram[2]]])/3\n","        #embedding for fourgrams\n","        if len(ngram) == 4:\n","            out[ngrams2id[ngram]]= (out[ngrams2id[ngram[0]]]+out[ngrams2id[ngram[1]]]+out[ngrams2id[ngram[2]]]+out[ngrams2id[ngram[3]]])/4               \n","    print('out_of_vocab characters: %d' % (out_of_vocab) )         \n","    return out,out_of_vocab\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"r_rZ15YoKhtD","colab":{}},"cell_type":"code","source":["# Retriev word_embeddings and PAD word id from Saved pkl File\n","def ngrams_pretrain_embed():\n","    FILE_FOR_NGRAM = 'All_datasets'\n","    ngrams2id = pickle.load(open(root_path+'Datasets/output_{}_ngram2id.pkl'.format(FILE_FOR_NGRAM), \"rb\"))[\"ngrams2id\"]    \n","    \n","    # get ngrams2ids's pretrained embeddings\n","    word_embeddings, out_of_vocab = reading_pretrained_embeddings(root_path+'embedding/character.vec', ngrams2id)\n","    PAD_ID = ngrams2id[PAD]\n","    return ngrams2id,word_embeddings,PAD_ID"],"execution_count":0,"outputs":[]},{"metadata":{"id":"57e2CoXaatzX","colab_type":"code","outputId":"e4383950-8cbe-49f8-8afe-bb928e1ac08d","executionInfo":{"status":"ok","timestamp":1556046862406,"user_tz":-120,"elapsed":18837,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["ngrams2id,word_embeddings,PAD_ID = ngrams_pretrain_embed()\n","print(len(word_embeddings))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["pretraine embedding files reading finished ...\n","out_of_vocab characters: 380\n","1773043\n"],"name":"stdout"}]},{"metadata":{"id":"AKFMS9NHbIcc","colab_type":"code","outputId":"4db72bee-09ce-4a40-b019-da7db6e7cded","executionInfo":{"status":"ok","timestamp":1556047095285,"user_tz":-120,"elapsed":11375,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["#Retriev train, test data from pkl files\n","FILE_NAME = 'pku'\n","\n","TRAIN_FILE_ADD = root_path+'Datasets/output_{}_train_test.pkl'.format(FILE_NAME)\n","\n","train_test_file = pickle.load(open(TRAIN_FILE_ADD, \"rb\"))\n","\n","subset_train = 1000 # Put number of subset data you want for train\n","subset_dev= 500\n","subset_test= 1000\n","\n","test_x = train_test_file[\"test_x\"][slice(0, subset_test)]\n","test_y = train_test_file[\"test_y\"][slice(0, subset_test)]\n","\n","PAD_ID = ngrams2id[PAD]\n","\n","\n","# all_datasets = 'pku'\n","# TEST_FILENAME = root_path+'Datasets/cleaned_data/{}/raw/cleaned_no_space_test.utf8'.format(all_datasets) # Address of no sace test file.\n","# test_x = get_test_data(TEST_FILENAME, ngrams2id, usebigram=True, usetrigram = True,usefourgram = True)#[slice(0, subset_test)]\n","\n","print(\"size of test data ....\")\n","print(len(test_x))\n","print(len(test_x[0]))\n","print(len(test_x[0][0]))\n","\n","print(len(test_y))\n","print(len(test_y[0]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["size of test data ....\n","1000\n","21\n","14\n","1000\n","21\n"],"name":"stdout"}]},{"metadata":{"id":"0GtiR4K3bI9b","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"AaGe2DVgbCSM","colab_type":"code","colab":{}},"cell_type":"code","source":["# ===-----------------------------------------------------------------------===\n","# Trainig Section\n","# ===-----------------------------------------------------------------------===\n","\n","def padding(X,padding_word):\n","\tmax_len = 0\n","\tfor x in X:\n","\t\tif len(x) > max_len:\n","\t\t\tmax_len = len(x)\n","\tpadded_X = np.ones((len(X), max_len), dtype=np.int32) * padding_word\n","\tfor i in range(len(X)):\n","\t\tfor j in range(len(X[i])):\n","\t\t\tpadded_X[i, j] = X[i][j]\n","\treturn padded_X\n","\n","def padding3(X,padding_word):\n","\tmax_len = 0\n","\tfor x in X:\n","\t\tif len(x) > max_len:\n","\t\t\tmax_len = len(x)\n","\tpadded_X = np.ones((len(X), max_len,14), dtype=np.int32) * padding_word\n","\tfor i in range(len(X)):\n","\t\tfor j in range(len(X[i])):\n","\t\t\tpadded_X[i, j] = X[i][j]\n","\treturn padded_X\n","\n","# function for calculating umber of hits and number of all predictions.\n","def number_of_batch_hits(y_batch_pred,y_batch_true):\n","    num_hits = 0\n","    num_all_chars = 0    \n","    for i in range(len(y_batch_true)):\n","     #   print(\"---------------------\")\n","        for j in range(len(y_batch_true[i])):\n","            num_all_chars = num_all_chars+1\n","            if y_batch_pred[i][j] == y_batch_true[i][j]:\n","               num_hits = num_hits+1    \n","    return num_hits, num_all_chars\n","\n","# ----------------- Add Summary Function ------------------------------------------\n","def add_summary(writer, name, value, global_step):\n","    summary = tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=value)])\n","    writer.add_summary(summary, global_step=global_step)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2lgdPZCJzoS5","colab_type":"code","outputId":"d3691a67-c547-4e35-8d0f-a7e118df08f9","executionInfo":{"status":"ok","timestamp":1556047175487,"user_tz":-120,"elapsed":1072,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["\n","VOCAB_SIZE =  word_embeddings.shape[0]\n","WORD_EMBEDDING_DIM = word_embeddings.shape[1]\n","\n","print(WORD_EMBEDDING_DIM)\n","print(VOCAB_SIZE)\n","\n","BATCH_SIZE = 16\n","HIDDEN_LAYER_DIM = 128\n","LEARNING_RATE = 0.01\n","NUM_CLASSES = 4\n","L2_REGU_LAMBDA=0.0001\n","NUM_LAYERS = 1\n","CLIP=5"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100\n","1773043\n"],"name":"stdout"}]},{"metadata":{"id":"ueEI2B4Nzo7R","colab_type":"code","colab":{}},"cell_type":"code","source":["# --------------------- Tensorflow part ---------------------------------------------------\n","\n","def create_tensorflow_model(vocab_size, embedding_size, hidden_layer_dim):\n","    print(\"Creating TENSORFLOW model\")\n","     \n","    # Inputs have (batch_size, timesteps) shape.\n","    X = tf.placeholder(dtype=tf.int32,shape=[None,None,14],name='input_x')   \n","    # Labels have (batch_size,) shape.\n","    labels = tf.placeholder(dtype=tf.int32,shape=[None,None],name='input_y')\n","    # dropout_keep_prob is a scalar.\n","    dropout_keep_prob=tf.placeholder(dtype=tf.float32,name='dropout_keep_prob')\n","    # Calculate sequence lengths to mask out the paddings later on.\n","    seq_length = tf.reduce_sum(tf.cast( tf.not_equal(X[:,:,2], tf.ones_like(X[:,:,2]) * PAD_ID), tf.int32), 1)\n","        \n","    with tf.variable_scope('embeddings', reuse=tf.AUTO_REUSE):\n","        embedding_matrix = tf.Variable(word_embeddings, dtype=tf.float32, name='embedding')\n","        #embedding_matrix = tf.get_variable(\"embeddings\", shape=[vocab_size, embedding_size])\n","        embeddings = tf.nn.embedding_lookup(embedding_matrix, X)\n","        embeddings = tf.reshape(embeddings,[tf.size(X[:,1,1]),-1,14*embedding_size])\n","    \n","    # embeddings shape (batch size, sentence length with padding, 1400)\n","    embeddings=tf.nn.dropout(tf.cast(embeddings,tf.float32),dropout_keep_prob)\n","    \n","    with tf.variable_scope('rnn_cell', reuse=tf.AUTO_REUSE):\n","            print ('rnn_cell is lstm')\n","            def lstm_cell():\n","                return tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_layer_dim), output_keep_prob=dropout_keep_prob)\n","            \n","            def gru_cell():\n","                return tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(hidden_layer_dim), output_keep_prob=dropout_keep_prob)\n","\n","            \n","            stacked_fw_lstm = tf.nn.rnn_cell.MultiRNNCell(\n","                [gru_cell() for _ in range(NUM_LAYERS)])\n","            \n","            stacked_bw_lstm = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell() for _ in range(NUM_LAYERS)])\n","        \n","    with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):                        \n","            (forward_output,backword_output),_=tf.nn.bidirectional_dynamic_rnn(\n","                cell_fw=stacked_fw_lstm,\n","                cell_bw=stacked_bw_lstm,\n","                inputs=embeddings,\n","                sequence_length=seq_length,\n","                dtype=tf.float32\n","            )\n","            outputBidirection=tf.concat([forward_output,backword_output],axis=2)\n","            print ('outputBidirection is ok')    \n","            \n","    print ('Loss Starts ....')\n","    with tf.variable_scope('loss', reuse=tf.AUTO_REUSE):\n","        output=layers.fully_connected(\n","            inputs=outputBidirection,\n","            num_outputs=NUM_CLASSES,\n","            activation_fn=None            \n","            )\n","        print ('output is ok ....')\n","        #crf\n","        log_likelihood, transition_params = crf.crf_log_likelihood(\n","            output, labels, seq_length)\n","        print ('crf_log_likelihood is ok ....')\n","        \n","        loss = tf.reduce_mean(-log_likelihood)\n","        print ('loss crf_log_likelihood is ok ....')\n","    \n","    print ('train-op Starts ....')\n","    with tf.variable_scope('train_op', reuse=tf.AUTO_REUSE):\n","        optimizer=tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n","        print ('optimizer is ok ....')\n","    \n","        tvars=tf.trainable_variables()\n","        print ('tvars is ok ....')\n","    \n","        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tvars if v.get_shape().ndims > 1])\n","        print ('l2_loss is ok ....')\n","        \n","        loss=loss+L2_REGU_LAMBDA*l2_loss\n","        print ('loss L2 is ok ....')\n","        \n","        grads,_=tf.clip_by_global_norm(tf.gradients(loss,tvars),CLIP)\n","        print ('grads is ok ....')\n","        \n","        train_op=optimizer.apply_gradients(zip(grads,tvars))\n","        print ('train_op apply_gradients is ok ....')\n","               \n","    return X, labels, output, train_op, dropout_keep_prob, loss, transition_params, seq_length\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gqunUjASdXms","colab_type":"code","outputId":"a0d5a6fe-2485-49cc-cc29-5b95ac2487c5","executionInfo":{"status":"ok","timestamp":1556047186429,"user_tz":-120,"elapsed":6266,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":717}},"cell_type":"code","source":["# create tensorflow model\n","X, labels, output, train_op, dropout_keep_prob, loss, transition_params, seq_length = create_tensorflow_model(VOCAB_SIZE, WORD_EMBEDDING_DIM, HIDDEN_LAYER_DIM)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Creating TENSORFLOW model\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-14-0e2692f053a6>:21: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","rnn_cell is lstm\n","WARNING:tensorflow:From <ipython-input-14-0e2692f053a6>:29: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-14-0e2692f053a6>:33: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-14-0e2692f053a6>:26: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-14-0e2692f053a6>:44: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","outputBidirection is ok\n","Loss Starts ....\n","output is ok ....\n","crf_log_likelihood is ok ....\n","loss crf_log_likelihood is ok ....\n","train-op Starts ....\n","optimizer is ok ....\n","tvars is ok ....\n","l2_loss is ok ....\n","loss L2 is ok ....\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["grads is ok ....\n","train_op apply_gradients is ok ....\n"],"name":"stdout"}]},{"metadata":{"id":"2WVOJYpdokPi","colab_type":"code","outputId":"e7666358-58b2-4b76-cdbc-129adb1a1176","executionInfo":{"status":"error","timestamp":1556047216256,"user_tz":-120,"elapsed":8754,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":2060}},"cell_type":"code","source":["#Restore Saved Tensorflow model.\n","import tensorflow as tf\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","    new_saver = tf.train.import_meta_graph( root_path+'best_model_path85.25236692748163/model.ckpt.meta')\n","    new_saver.restore(sess, root_path+'best_model_path85.25236692748163/model.ckpt')\n","    #saver.restore(sess, root_path+'best_model_path/model.ckpt')\n","    print(\"Model restored.\")\n","    \n","    #for tensor in tf.get_default_graph().get_operations():\n","     #   print(tensor.name)\n","    \n","    test_hit_all=0\n","    test_all_char=0        \n","    test_pred = []\n","    for i in range(0, len(test_x), BATCH_SIZE):\n","        batch_x = test_x[slice(i, i + BATCH_SIZE)]\n","        batch_x = padding3(batch_x, PAD_ID)\n","\n","        lengths,unary_scores,transition_param_other = sess.run(\n","            [seq_length,output,transition_params], feed_dict = {X:batch_x, dropout_keep_prob:1.0})\n","        predict=[]\n","        for unary_score,length in zip(unary_scores,lengths):\n","            if length > 0 :\n","                viterbi_sequence, _=crf.viterbi_decode(unary_score[:length],transition_param_other)\n","                predict.append(viterbi_sequence)\n","            else:\n","                predict.append(\"\")\n","            #print('sentence length : %d' % (length))\n","            #print(viterbi_sequence)\n","        test_pred += predict          \n","        \n","        test_batch_hit,test_batch_char = number_of_batch_hits(predict, test_y[slice(i, i + BATCH_SIZE)])            \n","        print('test_batch_hit:%f test_batch_char:%f batch_accuracy:%f' % (test_batch_hit, test_batch_char,(test_batch_hit/test_batch_char)*100 ))            \n","        test_hit_all+=test_batch_hit\n","        test_all_char+=test_batch_char\n","\n","    print('test_hit:%f test_all_char:%f accuracy:%f' % (test_hit_all,test_all_char,(test_hit_all/test_all_char)*100 ))\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/NLPHW1/best_model_path85.25236692748163/model.ckpt\n","Model restored.\n"],"name":"stdout"},{"output_type":"error","ename":"FailedPreconditionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value loss/transitions\n\t [[{{node loss/transitions}}]]\n\t [[{{node loss/fully_connected/BiasAdd}}]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-f56dffb5553a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         lengths,unary_scores,transition_param_other = sess.run(\n\u001b[0;32m---> 21\u001b[0;31m             [seq_length,output,transition_params], feed_dict = {X:batch_x, dropout_keep_prob:1.0})\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0munary_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value loss/transitions\n\t [[node loss/transitions (defined at <ipython-input-14-0e2692f053a6>:59) ]]\n\t [[node loss/fully_connected/BiasAdd (defined at <ipython-input-14-0e2692f053a6>:54) ]]\n\nCaused by op 'loss/transitions', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-6e46d071fd1a>\", line 1, in <module>\n    X, labels, output, train_op, dropout_keep_prob, loss, transition_params, seq_length = create_tensorflow_model(VOCAB_SIZE, WORD_EMBEDDING_DIM, HIDDEN_LAYER_DIM)\n  File \"<ipython-input-14-0e2692f053a6>\", line 59, in create_tensorflow_model\n    output, labels, seq_length)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/crf/python/ops/crf.py\", line 254, in crf_log_likelihood\n    transition_params = vs.get_variable(\"transitions\", [num_tags, num_tags])\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1479, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1220, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 547, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 499, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 911, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1509, in _init_from_args\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py\", line 79, in variable_op_v2\n    shared_name=shared_name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 1425, in variable_v2\n    shared_name=shared_name, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value loss/transitions\n\t [[node loss/transitions (defined at <ipython-input-14-0e2692f053a6>:59) ]]\n\t [[node loss/fully_connected/BiasAdd (defined at <ipython-input-14-0e2692f053a6>:54) ]]\n"]}]},{"metadata":{"id":"GXLt6n_ZQ0HT","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"UCE-wjHfrT6i","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"sRMcZGLMRJUq","colab_type":"code","colab":{}},"cell_type":"code","source":["def ids_to_tag_file(test_pred,output_file):\n","    with codecs.open(output_file, 'w','utf-8') as f:\n","        for i in range(len(test_pred)):\n","            tag_list = ''\n","            for j in range(len(test_pred[i])):\n","                if test_pred[i][j]==0:\n","                    tag_list +='B'\n","                elif test_pred[i][j]==1:\n","                    tag_list+='I'\n","                elif test_pred[i][j]==2:\n","                    tag_list+='E'\n","                elif test_pred[i][j]==3:\n","                    tag_list+='S'\n","            #print(tag_list)\n","            if(i != len(test_pred)-1):\n","                f.write(''.join(tag_list).strip())            \n","                f.write('\\n') \n","            else:\n","                f.write(''.join(tag_list).strip())            \n","    print(\"result is ready ...\")\n","   \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K8_hlucBRKG5","colab_type":"code","outputId":"5290737c-26f8-48c1-8940-247e2475db88","executionInfo":{"status":"ok","timestamp":1555712272332,"user_tz":-120,"elapsed":2241,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["ids_to_tag_file(test_pred,output_file=root_path+'Datasets/cleaned_data/pku/bies/result.utf8')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["result is ready ...\n"],"name":"stdout"}]},{"metadata":{"id":"S70sqIKxRMJF","colab_type":"code","colab":{}},"cell_type":"code","source":["ALL_TAGS = {\"B\", \"I\", \"E\", \"S\"}\n","\n","def is_valid_prediction(prediction_iter, gold_iter):\n","    assert len(prediction_iter) == len(gold_iter), \"Prediction and gold have different lengths\"\n","\n","    prediction_tags = set()\n","    gold_tags = set()\n","    nr_line = 1\n","    for preds, gold in zip(prediction_iter, gold_iter):\n","        assert len(preds) == len(gold), \"Line \" + str(nr_line) + \": lengths mismatch\"\n","        prediction_tags.update(preds)\n","        gold_tags.update(gold)\n","        nr_line += 1\n","    \n","    prediction_tags = {t.upper() for t in prediction_tags}\n","    gold_tags = {t.upper() for t in gold_tags}\n","    \n","    assert len(gold_tags.difference(ALL_TAGS)) == 0, \"Unknown tag detected in gold\"\n","    assert len(prediction_tags.difference(ALL_TAGS)) == 0, \"Unknown tag detected in predictions\"\n","\n","\n","def score(prediction_iter, gold_iter, verbose=False):\n","    \"\"\"\n","    Returns the precision of the model's predictions w.r.t. the gold standard (i.e. the tags of the\n","    correct word segmentation).\n","\n","    :param prediction_iter: List of strings in the BIES format representing the model's predictions.\n","    :param gold_iter: List of strings in the BIES format representing the gold standard.\n","\n","    :return: precision [0.0, 1.0]\n","    \n","    Ex. predictions_iter = [\"BEBESBIIE\",\n","                            \"BIIIEBEBESS\"]\n","        gold_iter = [\"BEBIEBIES\",\n","                     \"BIIESBEBESS\"]\n","        output: 0.7\n","    \n","    The same result can be obtain by passing list of lists\n","    Ex. predictions_iter = [[\"B\", \"E\", \"B\", \"E\", \"S\", \"B\", \"I\", \"I\", \"E\"],\n","                            [\"B\", \"I\", \"I\", \"I\", \"E\", \"B\", \"E\", \"B\", \"E\", \"S\", \"S\"]]\n","        gold_iter = [[\"B\", \"E\", \"B\", \"I\", \"E\", \"B\", \"I\", \"E\", \"S\"],\n","                     [\"B\", \"I\", \"I\", \"E\", \"S\", \"B\", \"E\", \"B\", \"E\", \"S\", \"S\"]]\n","        output: 0.7\n","\n","    \n","    \"\"\"\n","    \n","    is_valid_prediction(prediction_iter, gold_iter)\n","\n","    right_predictions = 0\n","    wrong_predictions = 0\n","\n","    for prediction_sentence, gold_sentence in zip(prediction_iter, gold_iter):\n","        for prediction_tag, gold_tag in zip(prediction_sentence, gold_sentence):\n","            if prediction_tag == gold_tag:\n","                right_predictions += 1\n","            else:\n","                wrong_predictions += 1\n","\n","    precision = right_predictions / (right_predictions + wrong_predictions)\n","    if verbose:\n","        print(\"Precision:\\t\", precision)\n","\n","    return precision\n","\n","\n","def label_text_to_iter(file_path):\n","    iter_ = []\n","    with open(file_path) as f:\n","        for line in f:\n","            line = line.strip().upper()\n","            iter_.append(line)\n","    return iter_\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T09i31gLRPgg","colab_type":"code","outputId":"5c9ee1d5-a00f-4165-bcd7-d74efdfdc5ad","executionInfo":{"status":"error","timestamp":1555712280480,"user_tz":-120,"elapsed":1159,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/-XD-6qRYDvBg/AAAAAAAAAAI/AAAAAAAAAAc/KXU2VRoZa6s/s64/photo.jpg","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":435}},"cell_type":"code","source":["prediction_file = root_path+'Datasets/cleaned_data/pku/bies/result.utf8'\n","gold_file = root_path+'Datasets/cleaned_data/pku/bies/test.utf8'\n","\n","prediction_iter = []\n","score(label_text_to_iter(prediction_file), label_text_to_iter(gold_file), verbose=True)\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-d0697d876405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprediction_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_text_to_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_text_to_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-a729fe76bdd2>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(prediction_iter, gold_iter, verbose)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mis_valid_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mright_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-a729fe76bdd2>\u001b[0m in \u001b[0;36mis_valid_prediction\u001b[0;34m(prediction_iter, gold_iter)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_valid_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_iter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Prediction and gold have different lengths\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprediction_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Prediction and gold have different lengths"]}]},{"metadata":{"id":"kGOfJ_aorYIY","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}